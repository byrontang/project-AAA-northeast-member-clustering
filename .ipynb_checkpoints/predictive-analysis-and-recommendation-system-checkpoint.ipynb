{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAA Northeast Customer Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3511 entries, 0 to 3510\n",
      "Columns: 237 entries, Household Key to x5_Young City Solos\n",
      "dtypes: float64(236), int64(1)\n",
      "memory usage: 6.3 MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../project-AAA-northeast-member-clustering/data/processed_data.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Household Key', 'ERS Member Cost Year 3 sum',\n",
       "       'ERS Member Cost Year 3 mean', 'Length Of Residence mean',\n",
       "       'Number of Children mean', 'ERS Member Cost Year 2 sum',\n",
       "       'ERS Member Cost Year 2 mean', 'Cost 2014 sum', 'Cost 2016 sum',\n",
       "       'ERS ENT Count Year 3 sum', 'ERS ENT Count Year 3 mean',\n",
       "       'Mail Responder mean', 'Cost 2015 sum', 'PrimaryMember sum',\n",
       "       'ERS ENT Count Year 1 sum', 'ERS ENT Count Year 1 mean',\n",
       "       'Email Available mean', 'Income mean', 'ERS Member Cost Year 1 sum',\n",
       "       'ERS Member Cost Year 1 mean', 'Cost 2019 sum', 'Credit Ranges mean',\n",
       "       'Member Tenure Years mean', 'AssociateMember sum', 'Cost 2018 sum',\n",
       "       'Total Cost sum', 'Member Key count', 'Do Not Direct Mail Solicit mean',\n",
       "       'Cost 2017 sum', 'ERS ENT Count Year 2 sum'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "### Product Usage\n",
    "Only a small portion of households using the products. The most used product has lower than 30% of the usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_list = ['FSV CMSI Flag', 'FSV Credit Card Flag', 'FSV Deposit Program Flag', \n",
    "                'FSV Home Equity Flag', 'FSV ID Theft Flag', 'FSV Mortgage Flag',\n",
    "                'INS Client Flag', 'TRV Globalware Flag', 'New Mover Flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Percent of Usage</th>\n",
       "      <th>Count of Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>INS Client Flag</th>\n",
       "      <td>0.281686</td>\n",
       "      <td>989.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRV Globalware Flag</th>\n",
       "      <td>0.179151</td>\n",
       "      <td>629.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FSV Credit Card Flag</th>\n",
       "      <td>0.136144</td>\n",
       "      <td>478.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FSV CMSI Flag</th>\n",
       "      <td>0.082598</td>\n",
       "      <td>290.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FSV ID Theft Flag</th>\n",
       "      <td>0.058103</td>\n",
       "      <td>204.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Mover Flag</th>\n",
       "      <td>0.054400</td>\n",
       "      <td>191.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FSV Deposit Program Flag</th>\n",
       "      <td>0.006551</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FSV Mortgage Flag</th>\n",
       "      <td>0.003133</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FSV Home Equity Flag</th>\n",
       "      <td>0.000854</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Percent of Usage  Count of Usage\n",
       "INS Client Flag                   0.281686           989.0\n",
       "TRV Globalware Flag               0.179151           629.0\n",
       "FSV Credit Card Flag              0.136144           478.0\n",
       "FSV CMSI Flag                     0.082598           290.0\n",
       "FSV ID Theft Flag                 0.058103           204.0\n",
       "New Mover Flag                    0.054400           191.0\n",
       "FSV Deposit Program Flag          0.006551            23.0\n",
       "FSV Mortgage Flag                 0.003133            11.0\n",
       "FSV Home Equity Flag              0.000854             3.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_product_pct = pd.DataFrame({'Percent of Usage': df[product_list].mean()}\n",
    "                             ).join(pd.DataFrame({'Count of Usage': df[product_list].sum()}))\n",
    "df_product_pct.sort_values(['Percent of Usage'], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 45% of the households do not use any products from AAA Northeast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    1589\n",
       "1.0    1220\n",
       "2.0     539\n",
       "3.0     135\n",
       "4.0      25\n",
       "5.0       3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of products bought by household\n",
    "df[product_list].sum(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4525776132156081"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1589/3511"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Analysis\n",
    "We could use classification models to predict the probability of an household buying a product on the products with more than 1% of household usage. The models build on these products should have be better in generalization than other low-usage products.\n",
    "\n",
    "For all products with over 5% user, apply the following process for modeling:\n",
    "- Upsampling\n",
    "- Create training & test set based on upsampled dataset\n",
    "- Grid search using Decision Tree (Set the max of depth to 10 instead of None to avoid overfitting)\n",
    "- Apply bagging with 100 estimators using decision tree and best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== FSV CMSI Flag ====\n",
      "1.0    3221\n",
      "0.0    3221\n",
      "Name: FSV CMSI Flag, dtype: int64\n",
      "\n",
      "== Decision Tree ==\n",
      "Best Parameters: {'max_depth': 12, 'criterion': 'gini', 'min_samples_split': 3}\n",
      "Accuracy on Training Set: 0.8540685355908583\n",
      "Accuracy on Test Set: 0.8634600465477114 AUC: 0.9166642592325099\n",
      "Accuracy on Original Dataset: 0.8174309313585872 AUC: 0.9451091436585339\n",
      "\n",
      "== Bagging ==\n",
      "Accuracy on Training Set: 0.9683679410052397\n",
      "Accuracy on Test Set: 0.9317300232738557 AUC: 0.9901247050893158\n",
      "Accuracy on Original Dataset: 0.9547137567644546 AUC: 0.9922587759209498\n",
      "Prediction Count:\n",
      "0.0    3086\n",
      "1.0     425\n",
      "dtype: int64 \n",
      "\n",
      "==== FSV Credit Card Flag ====\n",
      "1.0    3033\n",
      "0.0    3033\n",
      "Name: FSV Credit Card Flag, dtype: int64\n",
      "\n",
      "== Decision Tree ==\n",
      "Best Parameters: {'max_depth': 12, 'criterion': 'gini', 'min_samples_split': 2}\n",
      "Accuracy on Training Set: 0.744233280601357\n",
      "Accuracy on Test Set: 0.7413509060955519 AUC: 0.7980548189844456\n",
      "Accuracy on Original Dataset: 0.750783252634577 AUC: 0.8631831582025888\n",
      "\n",
      "== Bagging ==\n",
      "Accuracy on Training Set: 0.9235366859027205\n",
      "Accuracy on Test Set: 0.8525535420098846 AUC: 0.9616690505334524\n",
      "Accuracy on Original Dataset: 0.8516092281401311 AUC: 0.975781742533664\n",
      "Prediction Count:\n",
      "0.0    2550\n",
      "1.0     961\n",
      "dtype: int64 \n",
      "\n",
      "==== FSV ID Theft Flag ====\n",
      "1.0    3307\n",
      "0.0    3307\n",
      "Name: FSV ID Theft Flag, dtype: int64\n",
      "\n",
      "== Decision Tree ==\n",
      "Best Parameters: {'max_depth': 12, 'criterion': 'gini', 'min_samples_split': 3}\n",
      "Accuracy on Training Set: 0.8300859854590502\n",
      "Accuracy on Test Set: 0.8284202569916855 AUC: 0.8731894821999077\n",
      "Accuracy on Original Dataset: 0.7405297636001139 AUC: 0.8913697622986299\n",
      "\n",
      "== Bagging ==\n",
      "Accuracy on Training Set: 0.9011529011529011\n",
      "Accuracy on Test Set: 0.8835978835978836 AUC: 0.9955048425209446\n",
      "Accuracy on Original Dataset: 0.8071774423241241 AUC: 0.9945332835281073\n",
      "Prediction Count:\n",
      "0.0    2630\n",
      "1.0     881\n",
      "dtype: int64 \n",
      "\n",
      "==== INS Client Flag ====\n",
      "1.0    2522\n",
      "0.0    2522\n",
      "Name: INS Client Flag, dtype: int64\n",
      "\n",
      "== Decision Tree ==\n",
      "Best Parameters: {'max_depth': 12, 'criterion': 'gini', 'min_samples_split': 2}\n",
      "Accuracy on Training Set: 0.6790582403965303\n",
      "Accuracy on Test Set: 0.6749256689791873 AUC: 0.7558757661480433\n",
      "Accuracy on Original Dataset: 0.7285673597265736 AUC: 0.8135499615516919\n",
      "\n",
      "== Bagging ==\n",
      "Accuracy on Training Set: 0.9653035935563816\n",
      "Accuracy on Test Set: 0.8206144697720515 AUC: 0.8884645607417885\n",
      "Accuracy on Original Dataset: 0.9202506408430646 AUC: 0.955389939613304\n",
      "Prediction Count:\n",
      "0.0    2612\n",
      "1.0     899\n",
      "dtype: int64 \n",
      "\n",
      "==== TRV Globalware Flag ====\n",
      "1.0    2882\n",
      "0.0    2882\n",
      "Name: TRV Globalware Flag, dtype: int64\n",
      "\n",
      "== Decision Tree ==\n",
      "Best Parameters: {'max_depth': 12, 'criterion': 'gini', 'min_samples_split': 2}\n",
      "Accuracy on Training Set: 0.6918237944268313\n",
      "Accuracy on Test Set: 0.7077189939288812 AUC: 0.7633187102830734\n",
      "Accuracy on Original Dataset: 0.6001139276559385 AUC: 0.8131318341242004\n",
      "\n",
      "== Bagging ==\n",
      "Accuracy on Training Set: 0.9232270657124269\n",
      "Accuracy on Test Set: 0.8404163052905465 AUC: 0.947260735605623\n",
      "Accuracy on Original Dataset: 0.8647109085730561 AUC: 0.9686809416266086\n",
      "Prediction Count:\n",
      "0.0    2493\n",
      "1.0    1018\n",
      "dtype: int64 \n",
      "\n",
      "==== New Mover Flag ====\n",
      "1.0    3320\n",
      "0.0    3320\n",
      "Name: New Mover Flag, dtype: int64\n",
      "\n",
      "== Decision Tree ==\n",
      "Best Parameters: {'max_depth': 12, 'criterion': 'gini', 'min_samples_split': 3}\n",
      "Accuracy on Training Set: 0.894386955158357\n",
      "Accuracy on Test Set: 0.8983433734939759 AUC: 0.9432122314559443\n",
      "Accuracy on Original Dataset: 0.9171176303047565 AUC: 0.9673153346369773\n",
      "\n",
      "== Bagging ==\n",
      "Accuracy on Training Set: 0.9832454819277109\n",
      "Accuracy on Test Set: 0.9728915662650602 AUC: 0.9974041497314559\n",
      "Accuracy on Original Dataset: 0.9706636285958417 AUC: 0.9989836308585126\n",
      "Prediction Count:\n",
      "0.0    3221\n",
      "1.0     290\n",
      "dtype: int64 \n",
      "\n",
      "Wall time: 8min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "product_list_to_predict = ['FSV CMSI Flag', 'FSV Credit Card Flag', 'FSV ID Theft Flag',\n",
    "                           'INS Client Flag', 'TRV Globalware Flag', 'New Mover Flag']\n",
    "df_prob = pd.DataFrame()\n",
    "for prod in product_list_to_predict:\n",
    "    # Up Sampling\n",
    "    # Separate majority and minority classes\n",
    "    df_majority = df[df[prod]==0]\n",
    "    df_minority = df[df[prod]==1]\n",
    "\n",
    "    # Upsample minority class\n",
    "    df_minority_upsampled = resample(df_minority, \n",
    "                                     replace=True,     # sample with replacement\n",
    "                                     n_samples=len(df_majority),    # to match majority class\n",
    "                                     random_state=72) # reproducible results\n",
    "\n",
    "    # Combine majority class with upsampled minority class\n",
    "    df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "    # Display new class counts\n",
    "    print('====', prod, '====')\n",
    "    print(df_upsampled[prod].value_counts())\n",
    "    \n",
    "    # X - exclude total costs as it will be used for clustering\n",
    "    cols = [x for x in df_upsampled.columns if (x not in product_list\n",
    "                                               ) & (x not in ['Household Key', 'Total Cost sum'])]\n",
    "    X = df_upsampled[cols]\n",
    "\n",
    "    # y\n",
    "    y = df_upsampled[prod]\n",
    "        \n",
    "    # Training and testing sets    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=0.2,\n",
    "                                                        stratify=y,\n",
    "                                                        random_state=75)\n",
    "    \n",
    "    # Train Decision Tree\n",
    "    param_grid = {'criterion': ['gini', 'entropy'],\n",
    "                  'max_depth': [8, 10, 12],\n",
    "                  'min_samples_split': [2, 3, 4]}\n",
    "    \n",
    "    gs_dt = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)\n",
    "    gs_dt.fit(X_train, y_train)\n",
    "    print(\"\\n== Decision Tree ==\")\n",
    "    print(\"Best Parameters:\", gs_dt.best_params_)\n",
    "    print(\"Accuracy on Training Set:\", gs_dt.best_score_)\n",
    "\n",
    "    y_pred_prob = gs_dt.predict_proba(X_test)[:,1]\n",
    "    print(\"Accuracy on Test Set:\", gs_dt.score(X_test, y_test), \n",
    "          \"AUC:\", roc_auc_score(y_test, y_pred_prob))\n",
    "    \n",
    "    y_pred_prob = gs_dt.predict_proba(df[cols])[:,1]\n",
    "    print(\"Accuracy on Original Dataset:\", gs_dt.score(df[cols], df[prod]), \n",
    "          \"AUC:\", roc_auc_score(df[prod], y_pred_prob))\n",
    "    \n",
    "    # Train Bagging \n",
    "    dt = DecisionTreeClassifier(criterion=gs_dt.best_estimator_.criterion,\n",
    "                                max_depth=gs_dt.best_estimator_.max_depth,\n",
    "                                min_samples_split=gs_dt.best_estimator_.min_samples_split)\n",
    "    bg =  BaggingClassifier(dt, n_estimators = 100)\n",
    "    bg.fit(X_train, y_train)\n",
    "    print(\"\\n== Bagging ==\")\n",
    "    print(\"Accuracy on Training Set:\", bg.score(X_train, y_train))\n",
    "\n",
    "    y_pred_prob = bg.predict_proba(X_test)[:,1]\n",
    "    print(\"Accuracy on Test Set:\", bg.score(X_test, y_test),\n",
    "          \"AUC:\", roc_auc_score(y_test, y_pred_prob))\n",
    "    \n",
    "    y_pred_prob = bg.predict_proba(df[cols])[:,1]\n",
    "    print(\"Accuracy on Original Dataset:\", bg.score(df[cols], df[prod]),\n",
    "          \"AUC:\", roc_auc_score(df[prod], y_pred_prob))\n",
    "    \n",
    "    y_pred = bg.predict(df[cols])\n",
    "    print(\"Prediction Count:\")\n",
    "    print(pd.Series(y_pred).value_counts(), \"\\n\")\n",
    "    \n",
    "    df_prob[prod] = y_pred_prob\n",
    "\n",
    "df_prob.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With high AUC on predicting all products in the original dataset, we can be confident that the probability of purchsing would be a good reference on current and potential buyers of each product.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation System\n",
    "For the products with usage less than 5%, we can further use recommendation system to generate the probability of buying, based on the probabilities of top 3 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'surprise'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-173-0a688b1dd570>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSVD\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Load the movielens-100k dataset (download it if needed).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'surprise'"
     ]
    }
   ],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "# Load the movielens-100k dataset (download it if needed).\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "# Use the famous SVD algorithm.\n",
    "algo = SVD()\n",
    "\n",
    "# Run 5-fold cross-validation and print results.\n",
    "cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Cost in Next 12 Months\n",
    "Now we have the year cost, we can use the cost in year n-1 to predict the cost in n. First we have to rearrange the yearly cost columns to only two columns, cost last year and cost this year. Eventually, we can use cost in 2019 to predict cost in 2020.\n",
    "\n",
    "If the prediction result is not ideal, try using year n-2 and year n-1 to predict year n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3511 entries, 875 to 100079136\n",
      "Data columns (total 40 columns):\n",
      "ERS Member Cost Year 3 sum         3511 non-null float64\n",
      "Length Of Residence mean           3511 non-null float64\n",
      "Number of Children mean            3511 non-null float64\n",
      "ERS Member Cost Year 2 sum         3511 non-null float64\n",
      "ERS ENT Count Year 3 sum           3511 non-null float64\n",
      "Mail Responder mean                3511 non-null float64\n",
      "PrimaryMember sum                  3511 non-null float64\n",
      "Email Available mean               3511 non-null float64\n",
      "Income mean                        3511 non-null float64\n",
      "Cost 2019 sum                      3511 non-null float64\n",
      "Credit Ranges mean                 3511 non-null float64\n",
      "Member Tenure Years mean           3511 non-null float64\n",
      "AssociateMember sum                3511 non-null float64\n",
      "Member Key count                   3511 non-null float64\n",
      "Do Not Direct Mail Solicit mean    3511 non-null float64\n",
      "ERS ENT Count Year 2 sum           3511 non-null float64\n",
      "x0_1746.0                          3511 non-null category\n",
      "x0_1867.0                          3511 non-null category\n",
      "x0_1905.0                          3511 non-null category\n",
      "x0_2769.0                          3511 non-null category\n",
      "x0_2802.0                          3511 non-null category\n",
      "x0_2804.0                          3511 non-null category\n",
      "x0_2806.0                          3511 non-null category\n",
      "x0_2807.0                          3511 non-null category\n",
      "x0_2809.0                          3511 non-null category\n",
      "x0_2812.0                          3511 non-null category\n",
      "x0_2813.0                          3511 non-null category\n",
      "x0_2814.0                          3511 non-null category\n",
      "x0_2816.0                          3511 non-null category\n",
      "x0_2817.0                          3511 non-null category\n",
      "x0_2818.0                          3511 non-null category\n",
      "x0_2822.0                          3511 non-null category\n",
      "x0_2823.0                          3511 non-null category\n",
      "x0_2825.0                          3511 non-null category\n",
      "x0_2827.0                          3511 non-null category\n",
      "x0_2828.0                          3511 non-null category\n",
      "x0_2829.0                          3511 non-null category\n",
      "x0_2830.0                          3511 non-null category\n",
      "x0_2831.0                          3511 non-null category\n",
      "x0_2832.0                          3511 non-null category\n",
      "dtypes: category(24), float64(16)\n",
      "memory usage: 710.8 KB\n"
     ]
    }
   ],
   "source": [
    "df_cost_prediction = df.drop(['Cost 2014 sum', 'Cost 2015 sum', 'Cost 2016 sum', \n",
    "                              'Cost 2017 sum', 'Cost 2018 sum', 'Total Cost sum', \n",
    "                              'ERS ENT Count Year 1 sum', 'ERS ENT Count Year 1 mean',\n",
    "                              'ERS ENT Count Year 2 mean', 'ERS ENT Count Year 3 mean', \n",
    "                              'ERS Member Cost Year 1 sum', 'ERS Member Cost Year 1 mean',\n",
    "                              'ERS Member Cost Year 2 mean', 'ERS Member Cost Year 3 mean'] + product_list, axis = 1)\n",
    "    # Categorical variables\n",
    "cat_cols = [x for x in df_cost_prediction.columns if x.startswith('x')]\n",
    "for col in cat_cols:\n",
    "    df_cost_prediction[col] = df_cost_prediction[col].astype('category')\n",
    "df_cost_prediction.iloc[:, :40].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3511.000000\n",
       "mean       29.002273\n",
       "std        55.264674\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%        58.850000\n",
       "max       584.170000\n",
       "Name: Cost 2019 sum, dtype: float64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cost_prediction['Cost 2019 sum'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_cost_prediction.drop(['Cost 2019 sum'], axis=1)\n",
    "y = df_cost_prediction[['Cost 2019 sum']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.3687712590419572\n",
      "RMSE: 44.99548182166654\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print(\"R-squared:\", lr.score(X_test, y_test))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings from using Lasso\n",
    "import warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.36705000867496\n",
      "RMSE: 45.056787462013325\n",
      "Wall time: 56.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'alpha': np.linspace(0, 10, 11)}\n",
    "lasso = Lasso(normalize=True)\n",
    "gs_lasso = GridSearchCV(lasso, param_grid, cv=5)\n",
    "gs_lasso.fit(X_train, y_train)\n",
    "y_pred = gs_lasso.predict(X_test)\n",
    "\n",
    "print(\"R-squared:\", gs_lasso.score(X_test, y_test))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.6807803324681604\n",
      "RMSE: 31.512361627559592\n",
      "Wall time: 38.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "br = BaggingRegressor(Lasso(), n_estimators=100)\n",
    "br.fit(X_train, np.ravel(y_train))\n",
    "y_pred = br.predict(X_test)\n",
    "\n",
    "print(\"R-squared:\", br.score(X_test, np.ravel(y_test)))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Number of Neighbors: {'n_neighbors': 8}\n",
      "Accuracy on Training Set: 0.584634448574969\n",
      "Accuracy on Test Set: 0.5966303270564915\n",
      "AUC: 0.6398888103096024\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'n_neighbors': np.arange(8, 20)}\n",
    "gs_kNN = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
    "gs_kNN.fit(X_train, y_train)\n",
    "print(\"Best Number of Neighbors:\", gs_kNN.best_params_)\n",
    "print(\"Accuracy on Training Set:\", gs_kNN.best_score_)\n",
    "\n",
    "y_pred_prob = gs_kNN.predict_proba(X_test)[:,1]\n",
    "print(\"Accuracy on Test Set:\", gs_kNN.score(X_test, y_test))\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'tol': 10, 'C': 9, 'max_iter': 1000}\n",
      "Accuracy on Training Set: 0.5861214374225526\n",
      "Accuracy on Test Set: 0.5827552031714569\n",
      "AUC: 0.6002868143957252\n",
      "Wall time: 17.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'C': [15-6, 1e-5, 1e-4], \n",
    "              'tol': [10, 1, 0.1], \n",
    "              'max_iter': [1000, 1500]}\n",
    "gs_lr = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "gs_lr.fit(X_train, y_train)\n",
    "print(\"Best Parameters:\", gs_lr.best_params_)\n",
    "print(\"Accuracy on Training Set:\", gs_lr.best_score_)\n",
    "\n",
    "y_pred_prob = gs_lr.predict_proba(X_test)[:,1]\n",
    "print(\"Accuracy on Test Set:\", gs_lr.score(X_test, y_test))\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM\n",
    "SVM takes too much time to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 5, 'n_estimators': 150, 'max_features': 'sqrt'}\n",
      "Accuracy on Training Set: 0.6042131350681537\n",
      "Accuracy on Test Set: 0.6214073339940536\n",
      "AUC: 0.6567735344963066\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'n_estimators':[50, 100, 150],\n",
    "              'max_depth': [3, 5], \n",
    "              'max_features': ['auto', 'sqrt']}\n",
    "rf = RandomForestClassifier()\n",
    "gs_rf = GridSearchCV(rf, param_grid, cv=5)\n",
    "gs_rf.fit(X_train, np.ravel(y_train))\n",
    "print(\"Best Parameters:\", gs_rf.best_params_)\n",
    "print(\"Accuracy on Training Set:\", gs_rf.best_score_)\n",
    "\n",
    "y_pred_prob = gs_rf.predict_proba(X_test)[:,1]\n",
    "print(\"Accuracy on Test Set:\", gs_rf.score(X_test, y_test))\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    565\n",
       "1.0    444\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = gs_rf.predict(X_test)\n",
    "pd.Series(y_pred).value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
